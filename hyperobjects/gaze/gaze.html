
<div class="portfolio"><center>
  <h1>Gaze tracking in VR</h1></center>
  <div align="left" class="maincontent">

 <br>
<center>
    <img width="45%" src="images/gazemoon2.png">
<!--<img width="45%" src="hyperobjects/gaze/pupil.png">-->
 </center>    
 
    <div align="left" style=" padding: 10%;">
Working with Dr. Kuniecki at the <span class='word' ><a href="http://eapl.psychologia.uj.edu.pl/collaborators/5/">Emotion&nbsp;and&nbsp;Perception&nbsp;Lab</a></span> (Institute of Psychology, Jagiellonian University), 
I developed calibration and validation modules for the <span class='word' ><a href="https://pupil-labs.com/">Pupil&nbsp;Labs</a></span>  eye-tracker in a VR headset (HTC Vive Pro).
I communicate with the tracker through <span class='word' ><a href="https://zeromq.org/">ZeroMQ</a></span> sockets. Finally, I embedded these modules within the <a href="https://www.psychopy.org/"><span class='word' >PsychoPy</span> software</a> to control the experimental procedures. 
 <br><br>

 <center> 
 <img class="mainimage" width="55%" src="hyperobjects/gaze/calibration.gif"><br>
Illustration of the calibration process, from <a href="https://docs.pupil-labs.com/">Pupil Labs documentation</a>.

</center><br><br>
<center>

<video  class="mainimage" width="55%" style="transform: rotate(180deg);"  autoplay loop muted playsinline>

    <source src="hyperobjects/gaze/anim.webm" type="video/webm" />

</video>
<!-- <img width="55%" style="transform: rotate(180deg);" src="hyperobjects/gaze/anim.gif"><br><div  style="width: 55%; text-align: left; margin: 0 auto;">--><br>My gaze in Pupil Core interface
</div>



<br><br>

</center>

<br><br>
<center>
<img class="mainimage" width="45%" src="images/vive.jpg">
</center>
</div>
</div>
<!--</div>-->

